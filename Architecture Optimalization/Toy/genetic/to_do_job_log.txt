TODO: tpr=0.5 -> fpr as a metric
    - Should there be a choice between this and auc, or should I try to combine them.
    - Actually, why not F-score? It seems to make more sense, as it looks at tpr=0.5, but also looks at fpr, tnr, fnr.

TODO: Momentum as a parameter (READ: hw they behave/conserve in keras with one-by-one epoch calls)
    - Not using, one-by-one epoch calls anymore, due to learning decay as parameter.
    - Since, different opt algorithms have different key-words for momentum (ex. beta_1, beta_2, momentum),
    how should I handle it?

22/06: Learning decay as a parameter:
    - Used Learning rate scheduler: still needs to change some it in __mutate in __Mutator.

TODO: Check the size of the training dataset: https://github.com/36000/cnn_colorflow/blob/master/lcurve.py
    - Need to clone from John.

22/06: Create log file support, instead of printing to console:
    - Done, prints to do bottom of the file.

22/06: No Two max_outs/dropouts in a row:
    - Done, however without possibility of different configs (ex. always two convs in a row, before maxout).

05/07: Use weights of the old network, while creating a new one.
    - 23/06 - add layer - Testing how different layers behave.
    - 28/06 - add layer - First approach, by creating three models, and then merging them.
    - 29/06 - add layer - Deleted first approach.
                            New Approach:
                                1. Create a new model.
                                2. Populate it with copies of old model.
                                3. Set weights to ones from last model + new weights for a new layer.
    - 01/07 - rmv layer - Added a function to remove a layer. Fails, if the layer is first one, or last one.
                            That's not the case in this research though. (First layer is always Activation,
                            last one is Dense w/ 2 neurons).
    - 02/07 - add layer - Implemented into mutating.
                            Fixed a bug when a Conv layer is added right before MaxOut before Flatten.
                            Easy fix though, can be done more efficiently when copying weights.
    - 03/07 - bth fn's  - Implemented remove into mutating.
                            Changed __init__ of network, so that it's easier to make new mutation combinations.
                            Made a function to find a weight index of first Dense layer.
    - 04/07 - bth fn's  - Implemented a function to copy weights from a middle of next dense layer.
    - 05/07 - bth fn's  - Fixed minor bugs, which resulted in incorrect shape.

02-05/07: Make it Maxout safe, when adding new layer. AKA implement limit so that maxpool cannot be added if previous/next layer is maxpool:
    - Fixed during problem above.

04-06/07: Bug. Sometimes training pops out NaNs. Understand why, fix. It's not optimizer, unless there's something wrong with SGD/lr combination.
    Nets:
        1:
            architecture: [((3, 3), 8), 128, 64]
            optimizer   : Adam
            activation  : relu
            callbacks   : EarlyStopping/default
        2:
            architecture: [((7, 7), 16), ((5, 5), 8), 'max', 32, 32, 'drop0.30']
            optimizer   : Adam
            activation  : relu
            callbacks   : EarlyStopping/default
        3:
            architecture: [((5, 5), 8), 'max', ((5, 5), 8), 'max', 64, 'drop0.30']
            optimizer   : Adam
            activation  : relu
            callbacks   : EarlyStopping/default
    Solved.

    Note for future:
        Basically using relu with cross_entropy doesn't work, since 0 is not in domain of cross_entropy, but in range of relu.
        Thus, even though the last activation was tanh, the problem was 0, appearing sometimes in the range of nn(since 0 -> 0 in tanh).
        Use SIGMOID whenever possible as last activation of nn, whenever possible.

TODO: Bug. Nan in loss. Now in middle of the epoch. Maybe something with divergent optimizer.
    Nets:
        1:
            architecture: [((3, 3), 16), 64, 64, 'drop0.70']
            optimizer   : Nadam, 0.001
            activation  : relu
            callbacks   : EarlyStopping/default

17/07: Investigate why dropout at the end of arch is legal. It's shouldn't be.
    - Done, added 'or j == len(architecture) - 1' in __init__ of network. Also added a test, in the test_network.

17/07: Make probability distributions for different choices in __mutate.
    - Done. Used numpy.random instead of random.

TODO: Limit depth/number of weights. Include a strict weight # cap.
    - 21/07 - Made a function to determine # of weights in the model.
    - 22/07 - Implemented in add_layer method. However, it doesn't have strict weight # cap. (Only a loose one, before adding a new layer)
    - 24/07 - Added a max depth requirement when generating a networks for first time.

23/07: To prevent overfitting. Save weights before training, then compare scores after fitting. If it's worse, keep the weights.
    - Done:
        1. Modified __init__ to add previous score and previous weights.
        2. Modified fit to update these variables.
        3. Modified score to determine which weights to keep based on score.

TODO: Split helper methods based on what they are helping with. Maybe even make a backend folder.
    - 22/07 - Methods to mutate have different .py file.

24/07: Split local variables to 2 files. One for files, one for constants.
    - Done. Put in a subdir.

24-25/07: Make comments for all the functions/methods
    - 24/07 - commented all of network functions.
    - 25/07 - comments on all of mutator, helpers_mutate functions.
    - 14/08 - comments on all of helpers/get_file_names/log_save. However, added private helpers for helpers_mutate, which won't have comments.

TODO: Instead of having random mutation, make a mutation depended on two parents, and make random choices based on them. Or combine it and use both (random and parents version).

30/07-01/08: Add a sequence of conv_max, instead of adding one layer at a time.
    - 30/07 - Basic function is written. Throws an error with copying over weights.
    - 31/07 - 1. Fixed an error. Not yet implemented.
              2. Added a mirror function of adding a dense layer, followed by a dropout.
                    // Note: Even if it will be added at the end, dropout will be dropped, due to Network __init__ restrictions.
              3. Added remove_conv_max function. Tested and working.
    - 01/08 - 1. Added remove_dense_drop function. Tested and working.

30/07: BUG. Cross-references are not working. in B: import A, in A: import B.
    - Done with helpers_mutate and Network.

TODO: Remake the code for helpers/mutator/network, so that non-random injections are supported
    - 14/08 - 1. Done for helpers_mutate. Checked Network, not needed.
              2. Done for creating new Networks in Mutator.
    - It can help debug the Nan in loss bug, which is extremely hard to reproduce (2-6h for single reproduction)
    - Still need an idea on what to do with mutations, since they're random right now.
